# Godex configuration template (YAML)
# Copy to ~/.config/godex/config.yaml or set GODEX_CONFIG

exec:
  model: gpt-5.2-codex
  instructions: "You are a helpful assistant."
  append_system_prompt: ""
  tool_choice: auto
  timeout: 90s
  allow_refresh: false
  auto_tools: false
  auto_tools_max_steps: 4
  mock: false
  mock_mode: echo
  web_search: false

client:
  base_url: https://chatgpt.com/backend-api/codex
  originator: codex_cli_rs
  user_agent: codex_cli_rs/0.0
  retry_max: 1
  retry_delay: 300ms

auth:
  path: "" # default: ~/.codex/auth.json
  refresh_url: https://auth.openai.com/oauth/token
  client_id: app_EMoamEEZ73f0CkXaXp7hrann
  scope: "openid profile email"

proxy:
  listen: 127.0.0.1:39001
  api_key: ""
  allow_any_key: false
  allow_refresh: false
  model: gpt-5.2-codex           # default model
  base_url: https://chatgpt.com/backend-api/codex  # default base URL
  
  # Multi-model support: list available models
  # If empty, uses single 'model' above
  models:
    - id: gpt-5.2-codex
    - id: chat-gpt-5-3
      # base_url: https://other-backend/api  # optional per-model URL
  originator: codex_cli_rs
  user_agent: godex/0.0
  auth_path: "" # default: ~/.codex/auth.json
  cache_ttl: 6h
  log_level: info
  log_requests: false

  keys_path: "" # default: ~/.codex/proxy-keys.json
  default_rate: 60/m
  default_burst: 10
  default_quota_tokens: 0

  stats_path: "" # empty disables history
  stats_summary: "" # default: ~/.codex/proxy-usage.json
  stats_max_bytes: 10MB
  stats_max_backups: 3

  events_path: "" # default: ~/.codex/proxy-events.jsonl
  events_max_bytes: 1MB
  events_max_backups: 3

  meter_window: "" # empty disables windowed reset
  admin_socket: "~/.godex/admin.sock"

  payments:
    enabled: false
    provider: l402
    token_meter_url: ""

  # Multi-backend support: route models to different LLM providers
  backends:
    codex:
      enabled: true
      base_url: https://chatgpt.com/backend-api/codex
      credentials_path: ""  # default: ~/.codex/auth.json
      native_tools: false   # true = full Codex prompt with shell/apply_patch/update_plan
    
    anthropic:
      enabled: false  # set to true to enable Claude models
      credentials_path: ""  # default: ~/.claude/.credentials.json
      default_max_tokens: 4096
    
    # Custom OpenAI-compatible backends
    custom:
      # Example: local Ollama
      # ollama:
      #   type: openai
      #   enabled: true
      #   base_url: "http://localhost:11434/v1"
      #   discovery: true  # auto-discover models via /v1/models
      
      # Example: Groq cloud
      # groq:
      #   type: openai
      #   enabled: true
      #   base_url: "https://api.groq.com/openai/v1"
      #   auth:
      #     type: api_key
      #     key_env: "GROQ_API_KEY"  # read from environment

      # Example: Google Gemini (OpenAI-compatible endpoint)
      # gemini:
      #   type: openai
      #   enabled: true
      #   base_url: "https://generativelanguage.googleapis.com/v1beta/openai"
      #   auth:
      #     type: api_key
      #     key_env: "GEMINI_API_KEY"  # or pass X-Provider-Key per request
      # (add routing pattern: gemini: ["gemini-"] and aliases: gemini: gemini-2.5-pro, flash: gemini-2.5-flash)

      # Example: vLLM with hard-coded models
      # vllm:
      #   type: openai
      #   enabled: true
      #   base_url: "http://gpu-server:8000/v1"
      #   discovery: false
      #   models:
      #     - id: "vllm/llama-70b"
      #       display_name: "Llama 70B (vLLM)"
      #     - id: "vllm/mixtral"
      #       display_name: "Mixtral 8x7B"
    
    routing:
      patterns:
        anthropic:
          - claude-
          - sonnet
          - opus
          - haiku
        codex:
          - gpt-
          - o1-
          - o3-
          - codex-
        # Add patterns for custom backends:
        # ollama:
        #   - llama-
        #   - phi-
        #   - qwen-
        # groq:
        #   - groq/*
      aliases:
        sonnet: claude-sonnet-4-5-20250929
        opus: claude-opus-4-5
        haiku: claude-haiku-4-5
        gemini: gemini-2.5-pro
        flash: gemini-2.5-flash
  
  # Per-backend metrics collection
  metrics:
    enabled: false          # set to true to enable metrics
    path: ""                # persist metrics to file (JSONL)
    log_requests: false     # log individual request details
